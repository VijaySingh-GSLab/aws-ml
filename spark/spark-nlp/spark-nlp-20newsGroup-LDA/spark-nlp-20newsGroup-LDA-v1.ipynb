{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c johnsnowlabs spark-nlp\n",
    "#!y\n",
    "#!pip install spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::120286446822:role/sagemaker-ml-pipeline'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.sql import SparkSession, SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-67-42.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2b08288d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f2b08290d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [['Peter is  100% Good person & living in Germany!'],\n",
    "             ['Paula is AlsO a good person!'],\n",
    "             ['She lives, in London. Living from long time']]\n",
    "\n",
    "text_data1 = [[\" senetence- 0 is @ having       Few Text!\"], \n",
    "            [\"sentence-1 this iS     cuda PRogram.\"],\n",
    "            [\"sent-2 program and &            RUNNING on GPu\"],\n",
    "            [\"sent-3 checkinh   the STemming and @tokenS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "+-----------------------------------------------+\n",
      "|text_raw                                       |\n",
      "+-----------------------------------------------+\n",
      "|Peter is  100% Good person & living in Germany!|\n",
      "|Paula is AlsO a good person!                   |\n",
      "|She lives, in London. Living from long time    |\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.createDataFrame(text_data).toDF('text_raw')\n",
    "\n",
    "print(df_spark.count())\n",
    "df_spark.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_input = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "+--------------------+--------------------+\n",
      "|                news|            category|\n",
      "+--------------------+--------------------+\n",
      "|From: Mamatha Dev...|    rec.sport.hockey|\n",
      "|From: mblawson@mi...|comp.sys.ibm.pc.h...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\"\"\"\n",
    "path = \"/home/ec2-user/SageMaker/aws-ml/spark/newsgroup_20_data.parquet\"\n",
    "df_spark = sqlContext.read.parquet(path)\n",
    "print(df_spark.count())\n",
    "df_spark.limit(2).show(truncate=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                news|            category|                text|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|From: Mamatha Dev...|    rec.sport.hockey|From: Mamatha Dev...|\n",
      "|From: mblawson@mi...|comp.sys.ibm.pc.h...|From: mblawson@mi...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_in = df_spark.withColumn(col_input, df_spark[\"news\"])\n",
    "#data_in = df_spark.withColumn(col_input, df_spark[\"text_raw\"])\n",
    "\n",
    "print(data_in.count())\n",
    "data_in.limit(2).show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, size\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_input = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    ".setInputCol(col_input)\\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['token']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True) \\\n",
    "     .setCleanupPatterns([r'[^a-zA-Z0-9 &]'])\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"stem\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\\\n",
    "      #.setStopWords([\"no\", \"without\"])\n",
    "      \n",
    "\"\"\"\n",
    "from sparknlp.annotator import PerceptronModel\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'lemmatized']) \\\n",
    "     .setOutputCol('pos')\n",
    "\"\"\"\n",
    "tokenassembler = TokenAssembler()\\\n",
    "    .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"clean_text\")\n",
    "\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setIncludeMetadata(False) # set to False to remove metadata\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    " documentAssembler, \n",
    " tokenizer,\n",
    " normalizer,\n",
    " stemmer,\n",
    " stopwords_cleaner,\n",
    " #tokenassembler,\n",
    " finisher\n",
    " ])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(col_input)\n",
    "pipelineModel = nlpPipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "+--------------------+--------------------+\n",
      "|            category|         col_nlp_arr|\n",
      "+--------------------+--------------------+\n",
      "|    rec.sport.hockey|[mamatha, devinen...|\n",
      "|comp.sys.ibm.pc.h...|[mblawsonmidwayec...|\n",
      "|talk.politics.mid...|[hilmierdsvsus, h...|\n",
      "|comp.sys.ibm.pc.h...|[guydaustinibmcom...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input data : data_in\n",
    "\n",
    "data_arr = pipelineModel.transform(data_in)\n",
    "\n",
    "data_arr = data_arr.withColumnRenamed(\"finished_cleanTokens\", \"col_nlp_arr\")\n",
    "data_arr = data_arr.select(\"category\", \"col_nlp_arr\")\n",
    "\n",
    "# filter out less data records\n",
    "#size_ = udf(lambda xs: len(xs), IntegerType())\n",
    "#data_arr = data_arr.filter( size_(data_arr.col_nlp_arr)  >= 40)\n",
    "\n",
    "\n",
    "new_data = data_arr.where(size(col(\"col_nlp_arr\")) >= 50)\n",
    "\n",
    "\n",
    "print(data_arr.count())\n",
    "data_arr.limit(4).show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_in, df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. tfidf\n",
    "coverting text data to ML features i.e tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nlp_arr = \"col_nlp_arr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "\n",
    "#data_arr = data_arr.where(size(col(\"col_nlp_arr\")) >= 20)\n",
    "\n",
    "cv = CountVectorizer(inputCol=col_nlp_arr, outputCol=\"cv_features\", minDF=5.0)\n",
    "cvmodel = cv.fit(data_arr)\n",
    "result_cv = cvmodel.transform(data_arr)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"cv_features\", outputCol=\"idf_features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv)\n",
    "\n",
    "\n",
    "#data_pp = result_tfidf.select('category', \"news\", \"features\")\n",
    "data_pp = result_tfidf\n",
    "\n",
    "print(type(data_pp))\n",
    "print(data_pp.count())\n",
    "data_pp.limit(4).show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvmodel.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del data_in, data_arr, df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 2\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='idf_features')\n",
    "\n",
    "lda_model = lda.fit(data_pp)\n",
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : idfModel, \n",
    "# retrieve topic words \n",
    "vocab = cvmodel.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "    return [vocab[token_id] for token_id in token_list]\n",
    "\n",
    "# converts word ids (the actual output for a topic by a topic model) into words\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  output words for each modelled topic with LDA model function describeTopics\n",
    "\n",
    "# output of lda_model.describeTopics : topic, termIndices, termWeights\n",
    "\n",
    "num_top_words = 7\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "\n",
    "topics.select('topic', 'topicWords').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.describeTopics(num_top_words).show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
