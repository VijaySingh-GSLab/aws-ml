{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spark-nlp\n",
    "#!pip install fastparquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "     .master('local[*]') \\\n",
    "     .appName('Spark NLP') \\\n",
    "     .config('spark.jars.packages') \\\n",
    "     .getOrCreate()\n",
    "\n",
    "spark\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-148-247.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2146322588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f214588f860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark\n",
    "sqlContext = SQLContext(spark)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext('local', 'PySPARK LDA Example')\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "df_spark = sqlContext.read.parquet(\"newsgroup_20_data.parquet\")\n",
    "print(df_spark.count())\n",
    "#df_spark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "d = df_spark.head(1)\n",
    "d = d[0]\n",
    "\n",
    "print(d.asDict()['category'])\n",
    "#print(d.asDict()['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = df_spark.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer,IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator# convert text column to spark nlp document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_input = \"news\"\n",
    "col_label = \"category\"\n",
    "col_nlp = 'col_nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "from utils import CUSTOM_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(sentence):\n",
    "    \n",
    "    # clean the punctuations\n",
    "    punc_re = r'[^a-zA-Z0-9 &]'\n",
    "    sentence = re.sub(punc_re, ' ', sentence)\n",
    "    \n",
    "    # tokens\n",
    "    arr = sentence.split()\n",
    "    \n",
    "    # remove white spaces\n",
    "    arr = [word.strip() for word in arr if word.isalpha() and len(word)>=4]\n",
    "    \n",
    "    arr = \" \".join(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "+--------------------+------------+--------------------+\n",
      "|                news|    category|             col_nlp|\n",
      "+--------------------+------------+--------------------+\n",
      "| agate!ames!purdu...|misc.forsale|agate ames purdue...|\n",
      "| agate!iat.holone...|   rec.autos|agate holonet psi...|\n",
      "+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = trainingData.limit(1000)\n",
    "#data = trainingData\n",
    "\n",
    "udf_text_cleaner = F.udf(text_cleaner, StringType())\n",
    "#udf_text_cleaner = F.udf(text_cleaner, ArrayType(elementType=StringType()))\n",
    "\n",
    "data_train_clean = data.withColumn(col_nlp, udf_text_cleaner(col_input))\n",
    "\n",
    "print(data_train_clean.count())\n",
    "data_train_clean.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol(col_input) \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['token']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel()\\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemma')\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('clean_lemma') \\\n",
    "     .setCaseSensitive(False) \\\n",
    "     .setStopWords(CUSTOM_STOP_WORDS)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           lemmatizer,\n",
    "           stopwords_cleaner,\n",
    "           finisher\n",
    "     ])\n",
    "\n",
    "pipeline2 = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           finisher\n",
    "           #lemmatizer,\n",
    "           #stopwords_cleaner\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "+------------+--------------------+--------------------+\n",
      "|    category|          normalized|                news|\n",
      "+------------+--------------------+--------------------+\n",
      "|misc.forsale|[[token, 1, 51, a...| agate!ames!purdu...|\n",
      "|   rec.autos|[[token, 1, 52, a...| agate!iat.holone...|\n",
      "+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline2.fit(data_train_clean)\n",
    "\n",
    "train_temp = pipeline_model.transform(data_train_clean)\n",
    "#pp_test_data = pipeline_model.transform(data_test_clean)\n",
    "\n",
    "pp_train_data = train_temp.select('category','normalized',col_input)\n",
    "\n",
    "print(pp_train_data.count())\n",
    "pp_train_data.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pp_train_data.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([Row(annotatorType='token', begin=1, end=51, result='agateamespurduementorccpurdueedusageccpurdueedukari', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=64, end=70, result='subject', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=77, end=79, result='meg', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=81, end=87, result='seagate', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=89, end=91, result='ide', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=93, end=96, result='hard', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=98, end=102, result='drive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=108, end=111, result='from', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=114, end=132, result='karisageccpurdueedu', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=139, end=142, result='kari', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=145, end=156, result='distribution', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=159, end=169, result='miscforsale', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=173, end=192, result='miscforsalecomputers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=197, end=209, result='purdueforsale', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=212, end=223, result='organization', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=226, end=231, result='purdue', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=233, end=242, result='university', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=244, end=252, result='computing', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=254, end=259, result='center', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=261, end=268, result='keywords', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=271, end=275, result='drive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=278, end=280, result='ide', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=283, end=285, result='meg', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=288, end=295, result='seagatei', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=297, end=300, result='have', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=302, end=302, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=308, end=310, result='meg', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=312, end=318, result='seagate', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=320, end=322, result='ide', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=324, end=328, result='drive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=330, end=331, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=333, end=336, result='sell', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=338, end=339, result='as', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=341, end=341, result='i', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=343, end=346, result='have', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=348, end=351, result='come', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=353, end=358, result='across', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=360, end=364, result='lines', metadata={'sentence': '0'}, embeddings=[])]),\n",
       "       list([Row(annotatorType='token', begin=1, end=52, result='agateiatholonetnetpsinntppsinntpmegatestmithrilalung', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=62, end=68, result='subject', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=71, end=72, result='re', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=76, end=80, result='geeko', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=84, end=86, result='waz', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=89, end=93, result='geico', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=95, end=103, result='annoyance', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=106, end=109, result='from', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=112, end=127, result='alungmegatestcom', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=132, end=136, result='aaron', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=138, end=141, result='lung', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=144, end=155, result='distribution', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=158, end=165, result='recautos', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=168, end=179, result='organization', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=182, end=189, result='megatest', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=191, end=201, result='corporation', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=203, end=207, result='lines', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=214, end=215, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=217, end=223, result='article', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=225, end=247, result='cwgfmonooseecnpurdueedu', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=259, end=283, result='rjwaderainbowecnpurdueedu', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=290, end=295, result='robert', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=297, end=297, result='j', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=300, end=303, result='wade', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=306, end=311, result='writes', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=314, end=315, result='if', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=318, end=320, result='you', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=322, end=325, result='want', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=327, end=328, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=330, end=334, result='annoy', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=336, end=340, result='geico', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=343, end=346, result='call', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=348, end=351, result='them', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=353, end=358, result='upgive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=363, end=366, result='fake', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=368, end=374, result='namebut', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=379, end=382, result='real', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=384, end=386, result='car', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=389, end=396, result='specsget', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=401, end=401, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=403, end=407, result='quote', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=409, end=411, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=413, end=416, result='then', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=418, end=421, result='tell', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=423, end=426, result='them', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=428, end=431, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=433, end=435, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=437, end=440, result='more', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=442, end=450, result='expensive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=452, end=455, result='than', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=457, end=460, result='your', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=463, end=469, result='current', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=472, end=476, result='state', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=478, end=489, result='farmallstate', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=492, end=500, result='insurance', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=504, end=507, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=509, end=512, result='will', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=514, end=518, result='still', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=520, end=523, result='send', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=525, end=527, result='you', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=529, end=533, result='quote', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=535, end=537, result='etc', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=540, end=543, result='then', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=546, end=548, result='you', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=550, end=552, result='can', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=554, end=557, result='tear', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=559, end=560, result='up', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=562, end=566, result='their', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=568, end=572, result='quote', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=574, end=576, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=578, end=582, result='stuff', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=584, end=585, result='it', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=587, end=588, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=590, end=592, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=594, end=600, result='prepaid', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=602, end=607, result='return', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=610, end=617, result='envelope', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=620, end=622, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=624, end=627, result='mail', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=629, end=630, result='it', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=632, end=635, result='back', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=637, end=638, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=640, end=643, result='them', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=647, end=654, result='actually', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=656, end=659, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=661, end=664, result='were', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=670, end=673, result='more', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=675, end=678, result='than', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=680, end=681, result='my', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=683, end=689, result='current', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=691, end=695, result='state', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=698, end=701, result='farm', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=703, end=707, result='rates', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=712, end=719, result='actually', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=722, end=724, result='ive', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=727, end=729, result='had', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=731, end=731, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=733, end=735, result='bad', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=737, end=741, result='habit', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=743, end=744, result='of', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=746, end=753, result='stuffing', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=755, end=755, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=757, end=761, result='whole', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=763, end=767, result='bunch', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=769, end=770, result='of', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=772, end=776, result='other', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=778, end=784, result='garbage', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=786, end=789, result='junk', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=791, end=794, result='mail', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=796, end=797, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=799, end=803, result='along', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=805, end=808, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=810, end=817, result='whatever', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=819, end=822, result='else', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=824, end=827, result='into', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=830, end=837, result='anybodys', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=841, end=847, result='prepaid', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=849, end=857, result='envelopes', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=859, end=863, result='until', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=865, end=868, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=870, end=875, result='almost', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=877, end=881, result='burst', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=885, end=885, result='i', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=887, end=893, result='believe', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=895, end=898, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=900, end=902, result='pay', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=904, end=910, result='postage', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=912, end=913, result='by', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=915, end=920, result='weight', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=923, end=925, result='heh', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=928, end=930, result='heh', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=933, end=935, result='heh', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=941, end=947, result='anyways', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=950, end=953, result='dont', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=956, end=959, result='tear', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=961, end=962, result='up', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=964, end=966, result='the', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=968, end=973, result='quotes', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=975, end=978, result='just', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=980, end=983, result='yeti', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=988, end=996, result='sometimes', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=998, end=1000, result='use', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1002, end=1006, result='their', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1008, end=1013, result='quotes', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1015, end=1016, result='or', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1018, end=1022, result='other', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1024, end=1032, result='insurance', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1034, end=1043, result='quotations', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1045, end=1046, result='as', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1048, end=1055, result='leverage', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1057, end=1058, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1060, end=1065, result='haggle', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1067, end=1069, result='for', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1071, end=1071, result='a', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1073, end=1077, result='lower', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1079, end=1082, result='rate', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1084, end=1092, result='elsewhere', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1096, end=1102, result='usually', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1104, end=1105, result='it', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1107, end=1111, result='works', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1113, end=1114, result='to', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1117, end=1120, result='your', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1123, end=1131, result='advantage', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1133, end=1134, result='if', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1137, end=1140, result='they', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1142, end=1144, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1146, end=1150, result='lower', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=1154, end=1158, result='aaron', metadata={'sentence': '0'}, embeddings=[])])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"normalized\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer , IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+------------+--------------------+--------------------+\n",
      "|    category|                news|            features|\n",
      "+------------+--------------------+--------------------+\n",
      "|misc.forsale| agate!ames!purdu...|(2193,[1,5,7,8,13...|\n",
      "|   rec.autos| agate!iat.holone...|(2193,[3,4,5,7,8,...|\n",
      "+------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def type_changer(sentence):\n",
    "    return sentence.split(\" \")\n",
    "\n",
    "udf_type_changer = F.udf(type_changer, ArrayType(elementType=StringType()))\n",
    "data_arr = data_train_clean.withColumn(\"col_nlp_arr\", udf_type_changer(col_nlp))\n",
    "\n",
    "# TF\n",
    "cv = CountVectorizer(inputCol=\"col_nlp_arr\", outputCol=\"raw_features\", vocabSize=5000, minDF=10.0)\n",
    "cvmodel = cv.fit(data_arr)\n",
    "result_cv = cvmodel.transform(data_arr)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv)\n",
    "\n",
    "pp_train_data = result_tfidf.select('category', col_input, \"features\")\n",
    "\n",
    "print(type(pp_train_data))\n",
    "pp_train_data.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = pp_train_data[['news','features']].map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -2377656.2476316197\n",
      "The upper bound on perplexity: 7.38978733968984\n"
     ]
    }
   ],
   "source": [
    "numTopics = 20 # number of topics\n",
    " \n",
    "lda = LDA(k=numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True,\n",
    "          maxIter = 10,           # number of iterations\n",
    "          learningDecay = 0.51,   # kappa, learning rate\n",
    "          learningOffset = 64.0,  # tau_0, larger values downweigh early iterations\n",
    "          subsamplingRate = 0.05, # mini batch fraction \n",
    "          )\n",
    " \n",
    "model = lda.fit(pp_train_data.select(\"features\"))\n",
    " \n",
    "ll = model.logLikelihood(pp_train_data)\n",
    "lp = model.logPerplexity(pp_train_data)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lperplexity = model.logPerplexity(pp_test_data)\n",
    "print(lperplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe topics.\n",
    "N = 3\n",
    "topics = model.describeTopics(N)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first topic\n",
    "model.describeTopics().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# show head()\n",
    "result_tfidf.show()\n",
    "\n",
    "\n",
    "# select columns\n",
    "df_model=result_tfidf.select('index','list_of_words','features')\n",
    "\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
