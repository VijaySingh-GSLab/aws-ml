{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spark-nlp\n",
    "#!pip install fastparquet \n",
    "#!pip install spark-nlp==2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col, size, length\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer,IndexToString\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "from custom_utils import CUSTOM_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.1\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "spark\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-10-164.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4fc52525f8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.1\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "df_spark = sqlContext.read.parquet(\"newsgroup_20_data.parquet\")\n",
    "print(df_spark.count())\n",
    "#df_spark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = df_spark.head(1)\n",
    "d = d[0]\n",
    "\n",
    "print(d.asDict()['category'])\n",
    "print(d.asDict()['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingData = df_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_input = \"news\"\n",
    "col_label = \"category\"\n",
    "col_nlp = 'col_nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(sentence):\n",
    "    \n",
    "    # clean the punctuations\n",
    "    punc_re = r'[^a-zA-Z0-9 &]'\n",
    "    sentence = re.sub(punc_re, ' ', sentence)\n",
    "    \n",
    "    # tokens\n",
    "    arr = sentence.split()\n",
    "    \n",
    "    # remove white spaces\n",
    "    # lowercase\n",
    "    # filter words having lenght <= 3\n",
    "    arr = [word.strip().lower() for word in arr if word.isalpha() and len(word)>=4]\n",
    "    \n",
    "    # remove starting 4 words as they are email id\n",
    "    arr = arr[20:-4]\n",
    "    \n",
    "    arr = \" \".join(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                news|            category|             col_nlp|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|From: Mamatha Dev...|    rec.sport.hockey|sure some bashers...|\n",
      "|From: mblawson@mi...|comp.sys.ibm.pc.h...|midway uoknor org...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data = df_spark.limit(10000)\n",
    "data = df_spark\n",
    "\n",
    "udf_text_cleaner = F.udf(text_cleaner, StringType())\n",
    "\n",
    "data_clean = data.withColumn(col_nlp, udf_text_cleaner(col_input))\n",
    "\n",
    "print(data_clean.count())\n",
    "data_clean.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. nlp pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    ".setInputCol(\"col_nlp\")\\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"stem\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\\\n",
    "      #.setStopWords([\"no\", \"without\"]) (e.g. read a list of words from a txt)\n",
    "      \n",
    "tokenassembler = TokenAssembler()\\\n",
    "    .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"clean_text\")\n",
    "\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setIncludeMetadata(False) # set to False to remove metadata\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    " documentAssembler, \n",
    " tokenizer,\n",
    " stemmer,\n",
    " stopwords_cleaner,\n",
    " #tokenassembler,\n",
    " finisher\n",
    " ])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"col_nlp\")\n",
    "pipelineModel = nlpPipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                news|            category|         col_nlp_arr|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|From: Mamatha Dev...|    rec.sport.hockey|[sure, basher, pe...|\n",
      "|From: mblawson@mi...|comp.sys.ibm.pc.h...|[midwai, uoknor, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data_in = data_clean.limit(1000)\n",
    "data_in = data_clean\n",
    "\n",
    "data_arr = pipelineModel.transform(data_in)\n",
    "\n",
    "data_arr = data_arr.withColumnRenamed(\"finished_cleanTokens\", \"col_nlp_arr\")\n",
    "data_arr = data_arr.select(\"news\", \"category\", \"col_nlp_arr\")\n",
    "\n",
    "data_arr.limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "18846\n",
      "+--------------------+--------------------+--------------------+\n",
      "|            category|                news|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|    rec.sport.hockey|From: Mamatha Dev...|(10936,[1,10,25,2...|\n",
      "|comp.sys.ibm.pc.h...|From: mblawson@mi...|(10936,[13,16,18,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF\n",
    "cv = CountVectorizer(inputCol=\"col_nlp_arr\", outputCol=\"raw_features\", minDF=10.0)\n",
    "cvmodel = cv.fit(data_arr)\n",
    "result_cv = cvmodel.transform(data_arr)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv)\n",
    "\n",
    "\n",
    "data_pp = result_tfidf.select('category', \"news\", \"features\")\n",
    "\n",
    "print(type(data_pp))\n",
    "print(data_pp.count())\n",
    "data_pp.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_spark, data_in, data_clean, data, data_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 20 # number of topics\n",
    " \n",
    "lda = LDA(k=numTopics, seed = 1, optimizer=\"online\", optimizeDocConcentration=True,\n",
    "          maxIter = 100,           # number of iterations\n",
    "          learningDecay = 0.51,   # kappa, learning rate\n",
    "          learningOffset = 64.0,  # tau_0, larger values downweigh early iterations\n",
    "          subsamplingRate = 0.05, # mini batch fraction \n",
    "          )\n",
    " \n",
    "model = lda.fit(data_pp)\n",
    "print(\"done....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = model.logLikelihood(data_pp)\n",
    "lp = model.logPerplexity(data_pp)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"modelling completed..!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. topic insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.describeTopics().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The topics described by their top-weighted terms:\")\n",
    "model.describeTopics(5).limit(6).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. topic assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = F.udf(lambda x: x.tolist().index(max(x)), IntegerType())\n",
    "\n",
    "\n",
    "data_lda = model.transform(data_pp)\n",
    "data_lda = data_lda.withColumn(\"topicID\", max_index(\"topicDistribution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_lda.count())\n",
    "data_lda.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "topicDistribution : list of topic weights (len==num_topics)\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. topic model assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_train_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topics = data_lda.select(\"category\", \"topicID\").toPandas()\n",
    "\n",
    "print(X_topics.shape)\n",
    "X_topics.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_metrics(df):\n",
    "    #print(df.head(2))\n",
    "    arr = df[\"topicID\"].value_counts()\n",
    "    max_topic = arr.index.values[0]\n",
    "    perc_dominance = arr[max_topic] / arr.sum()\n",
    "    \n",
    "    result = pd.Series(data=[int(max_topic), perc_dominance], index=[\"category_pred\", \"perc_dominance\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_topics.copy()\n",
    "#X = X_topics.head(10)\n",
    "\n",
    "X_label_mapping = X.groupby(\"category\").apply(topic_metrics).reset_index()\n",
    "X_label_mapping[\"category_pred\"] = X_label_mapping[\"category_pred\"].astype(\"int\")\n",
    "X_label_mapping[\"perc_dominance\"] = np.round(X_label_mapping[\"perc_dominance\"], 2)\n",
    "X_label_mapping = X_label_mapping.sort_values(by=[\"category_pred\", \"perc_dominance\"], ascending=[True, False])\n",
    "X_label_mapping = X_label_mapping.reset_index(drop=True)\n",
    "X_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mapper = {}\n",
    "for i in zip(X_label_mapping[\"category\"], X_label_mapping[\"category_pred\"]):\n",
    "    dict_mapper[str(i[1])] = i[0]\n",
    "dict_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mapper = {'0': 'rec.autos',\n",
    "                 '1': 'talk.politics.misc',\n",
    "                 '2': 'comp.os.ms-windows.misc',\n",
    "                 '3': 'sci.crypt',\n",
    "                 '5': 'comp.sys.ibm.pc.hardware',\n",
    "                 '9': 'rec.motorcycles',\n",
    "                 '12': 'sci.med',\n",
    "                 '14': 'alt.atheism',\n",
    "                 '15': 'rec.sport.baseball',\n",
    "                 '16': 'sci.electronics',\n",
    "                 '18': 'sci.space'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>topicID</th>\n",
       "      <th>category_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>8</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   category topicID    category_pred\n",
       "0          rec.sport.hockey       9  rec.motorcycles\n",
       "1  comp.sys.ibm.pc.hardware       8               NA"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_topics.copy()\n",
    "X[\"topicID\"] = X[\"topicID\"].astype(\"str\")\n",
    "X[\"category_pred\"] = X[\"topicID\"].replace(custom_mapper)\n",
    "\n",
    "#X[\"category_pred\"] = np.where(len(X[\"category_pred\"])<=3, \"NA\", X[\"category_pred\"])\n",
    "X[\"category_pred\"] = X[\"category_pred\"].apply(lambda val : \"NA\" if len(val)<=3 else val)\n",
    "\n",
    "print(X.shape)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = X[\"category_pred\"]\n",
    "y_true = X[\"category\"]\n",
    "#plot_cm()\n",
    "\n",
    "\n",
    "ac = accuracy_score( y_true, y_pred )\n",
    "all_vals = precision_recall_fscore_support(y_true, y_pred )\n",
    "precision = all_vals[0][1]\n",
    "recall = all_vals[1][1]\n",
    "fscore = all_vals[2][1]\n",
    "support = all_vals[3][1]\n",
    "\n",
    "text_print_plot = \\\n",
    "\"\"\"\n",
    "Confusion Matrix\n",
    "{} = {} \n",
    "{} = {}, {} = {}\n",
    "{} = {}\n",
    "{} = {}\n",
    "\"\"\".format(\n",
    "    'Accuracy', round(ac,2), \n",
    "    'Precision', round(precision,2), \n",
    "    'Recall', round(recall, 2),\n",
    "    'Fscore', round(fscore, 2),\n",
    "    'Support', support\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_print_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0e1601ce6bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_print_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_print_plot' is not defined"
     ]
    }
   ],
   "source": [
    "print(text_print_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix\n",
    "Accuracy = 0.2 \n",
    "Precision = 0.21, Recall = 0.52\n",
    "Fscore = 0.3\n",
    "Support = 799\n",
    "\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
